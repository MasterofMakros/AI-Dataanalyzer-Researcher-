# Compose file - version field removed (deprecated in Compose V2+)

# =============================================================================
# CONDUCTOR FULL STACK - SECURITY-HARDENED
# =============================================================================
# Verwendet .env Datei für alle Secrets!
# Kopiere .env.example → .env und passe die Werte an.
# =============================================================================

services:
  # ===========================================================================
  # INFRASTRUCTURE
  # ===========================================================================

  traefik:
    image: traefik:v3.2
    container_name: conductor-traefik
    restart: unless-stopped
    command:
      - "--api.dashboard=true"
      - "--api.insecure=false"                    # ✅ SECURITY FIX
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:8888"
    ports:
      - "8888:8888"
      # Dashboard nur über Tailscale/VPN erreichbar (nicht exposed!)
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 128M

  postgres:
    image: postgres:16-alpine
    container_name: conductor-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-n8n}
      POSTGRES_USER: ${POSTGRES_USER:-n8n}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}  # ✅ REQUIRED
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - conductor-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-n8n}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 256M

  redis:
    image: redis:7.4-alpine
    container_name: conductor-redis
    restart: unless-stopped
    command: 
      - "redis-server"
      - "--bind"
      - "0.0.0.0"
      - "--requirepass"
      - "${REDIS_PASSWORD:?REDIS_PASSWORD must be set}"  # ✅ REQUIRED
      - "--appendonly"
      - "no"
    volumes:
      - redis_data:/data
    networks:
      - conductor-net
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          memory: 128M

  # ===========================================================================
  # N8N WORKFLOW ENGINE
  # ===========================================================================

  n8n:
    image: n8nio/n8n:latest
    container_name: conductor-n8n
    restart: unless-stopped
    environment:
      # Basic
      - N8N_HOST=${N8N_HOST:-0.0.0.0}
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=${WEBHOOK_URL:-http://localhost:5678/}
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE:-Europe/Berlin}
      
      # Database
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=${POSTGRES_DB:-n8n}
      - DB_POSTGRESDB_USER=${POSTGRES_USER:-n8n}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
      
      # Security ✅
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY:?N8N_ENCRYPTION_KEY must be set}
      - N8N_SECURE_COOKIE=false
      
      # Features
      - N8N_AI_ENABLED=true
      - N8N_COMMUNITY_NODES_ENABLED=true
      
      # Performance
      - EXECUTIONS_MODE=regular
      - EXECUTIONS_DATA_PRUNE=true
      - EXECUTIONS_DATA_MAX_AGE=72
      - NODE_OPTIONS=--max-old-space-size=512
      - N8N_DIAGNOSTICS_ENABLED=false
    ports:
      - "5680:5678"
    volumes:
      - n8n_data:/home/node/.n8n
      - F:/:/mnt/data:ro  # Read-only Zugriff auf Datenpool
    networks:
      - conductor-net
    depends_on:
      postgres:
        condition: service_healthy
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.n8n.rule=Host(`n8n.local`)"
      - "traefik.http.services.n8n.loadbalancer.server.port=5678"
    deploy:
      resources:
        limits:
          memory: 768M

  # ===========================================================================
  # VECTOR DATABASE (RAG)
  # ===========================================================================

  qdrant:
    image: qdrant/qdrant:latest
    container_name: conductor-qdrant
    restart: unless-stopped
    ports:
      - "6335:6333"
      - "6336:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - conductor-net
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY:-}  # Optional
      - QDRANT__LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 1G

  # ===========================================================================
  # DOCUMENT PARSING
  # ===========================================================================

  tika:
    image: apache/tika:${TIKA_TAG:-3.2.3.0}
    container_name: conductor-tika
    restart: unless-stopped
    ports:
      - "9998:9998"
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 1G

  # ===========================================================================
  # LOCAL LLM
  # ===========================================================================

  ollama:
    image: ollama/ollama:latest
    container_name: conductor-ollama
    restart: unless-stopped
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - conductor-net
    # Für GPU: devices hinzufügen
    # devices:
    #   - /dev/dri:/dev/dri
    deploy:
      resources:
        limits:
          memory: 8G

  # ===========================================================================
  # BINARY FILE PROCESSING (NEU für Linux-Portabilität)
  # ===========================================================================

  # Audio-Transkription via Whisper (faster-whisper, WER 6-7%)
  # DEPRECATED: Wird durch whisperx ersetzt (siehe unten)
  whisper:
    image: fedirz/faster-whisper-server:${FASTER_WHISPER_CUDA_TAG:-latest-cuda}
    container_name: conductor-whisper
    restart: unless-stopped
    ports:
      - "9001:8000"
    environment:
      - WHISPER__MODEL=Systran/faster-whisper-base
      - WHISPER__DEVICE=cpu
      - WHISPER__COMPUTE_TYPE=float32
    volumes:
      - whisper_models:/root/.cache/huggingface
    networks:
      - conductor-net
    profiles:
      - legacy  # Nur mit --profile legacy starten
    deploy:
      resources:
        limits:
          memory: 4G

  # ===========================================================================
  # WHISPERX (EMPFOHLEN - Word-Level Timestamps + Speaker Diarization)
  # ===========================================================================
  # Benchmark: 70x Realtime, Word-Level Timestamps, Speaker Diarization
  # Aktivieren mit: USE_WHISPERX=true in feature_flags.py
  # Für Diarization: HF_TOKEN in .env setzen (huggingface.co Token)
  whisperx:
    build:
      context: ./infra/docker/whisperx-api
      dockerfile: Dockerfile
      args:
        WHISPERX_VERSION: ${WHISPERX_VERSION:-3.7.4}
    image: conductor-whisperx:latest
    container_name: conductor-whisperx
    restart: unless-stopped
    ports:
      - "9000:9000"
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-large-v3}
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
      - WHISPER_BATCH_SIZE=16
      - HF_TOKEN=${HF_TOKEN:-}  # Für Speaker Diarization
    volumes:
      - whisper_models:/root/.cache/huggingface
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 30s
      start_period: 120s
      retries: 3

  # FFmpeg-basierte Video/Audio API
  ffmpeg-api:
    image: jrottenberg/ffmpeg:7-ubuntu
    container_name: conductor-ffmpeg
    restart: unless-stopped
    entrypoint: ["tail", "-f", "/dev/null"]  # Hält Container am Laufen
    volumes:
      - F:/:/mnt/data:ro
      - ffmpeg_temp:/tmp/ffmpeg
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 1G

  # OCR für Bilder - Tesseract (schnell, 87% Genauigkeit)
  # DEPRECATED: Wird durch surya-ocr ersetzt (97.7% Accuracy)
  tesseract-ocr:
    image: jitesoft/tesseract-ocr:latest
    container_name: conductor-tesseract
    restart: unless-stopped
    entrypoint: ["tail", "-f", "/dev/null"]
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    profiles:
      - legacy  # Nur mit --profile legacy starten
    deploy:
      resources:
        limits:
          memory: 512M

  # ===========================================================================
  # SURYA OCR (EMPFOHLEN - 97.7% Accuracy + Layout-Analyse)
  # ===========================================================================
  # Benchmark: 97.7% Accuracy auf Invoices (vs. Tesseract 87%)
  # Features: 90+ Sprachen, Layout-Erkennung, Tabellen-Detection
  # Aktivieren mit: USE_SURYA_OCR=true in feature_flags.py
  surya-ocr:
    build:
      context: ./infra/docker/surya-ocr
      dockerfile: Dockerfile
    image: conductor-surya-ocr:latest
    container_name: conductor-surya-ocr
    restart: unless-stopped
    ports:
      - "9999:8000"
    environment:
      - SURYA_DEVICE=cuda
      - SURYA_BATCH_SIZE=4
      - SURYA_LANGS=de,en
    volumes:
      - huggingface_cache:/root/.cache/huggingface
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 6G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      start_period: 180s
      retries: 3

  # Parser Service - Python-basierte Datei-Parser (100% Docker)
  # parser-service:
  #   build:
  #     context: ./infra/docker/parser-service
  #     dockerfile: Dockerfile
  #   image: conductor-parser:latest
  #   container_name: conductor-parser
  #   restart: unless-stopped
  #   ports:
  #     - "8002:8000"
  #   volumes:
  #     - F:/:/mnt/data:ro
  #   networks:
  #     - conductor-net
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 1G
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # Scientific Parser Service - Tabular/Text Extraction
  scientific-parser:
    build:
      context: ./infra/docker/scientific-parser
      dockerfile: Dockerfile
    image: conductor-scientific-parser:latest
    container_name: conductor-scientific-parser
    restart: unless-stopped
    ports:
      - "8050:8050"
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8050/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # 7-Zip - Archive Processing (RAR, 7z, TAR, GZ ohne Entpacken)
  sevenzip:
    image: crazymax/7zip:latest
    container_name: conductor-7zip
    restart: unless-stopped
    entrypoint: ["tail", "-f", "/dev/null"]
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 512M

  # Metadata Extractor (ExifTool für RAW/PSD/EXE/AI/XCF/ICO)
  metadata-extractor:
    build:
      context: ./infra/docker/metadata-extractor
      dockerfile: Dockerfile
    image: conductor-metadata-extractor:latest
    container_name: conductor-metadata-extractor
    restart: unless-stopped
    ports:
      - "8015:8000"
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # DOCUMENT PROCESSOR (EMPFOHLEN - Unified Docling + Surya + GLiNER)
  # ===========================================================================
  # Benchmark-basierte Konsolidierung:
  # - Docling: 97.9% Table Accuracy (vs Tika 75%)
  # - Surya OCR: 97.7% Accuracy (vs Tesseract 87%)
  # - GLiNER: Zero-shot NER/PII Detection
  #
  # Ersetzt: neural-worker (CPU) + surya-ocr (separate)
  # Port: 8005 (gleich wie neural-worker für Kompatibilität)
  document-processor:
    build:
      context: ./infra/docker/document-processor
      dockerfile: Dockerfile
    image: conductor-document-processor:latest
    container_name: conductor-document-processor
    restart: unless-stopped
    ports:
      - "8005:8000"
    volumes:
      - F:/:/mnt/data:ro
      - huggingface_cache:/root/.cache/huggingface
      - lancedb_data:/data/lancedb
    networks:
      - conductor-net
    environment:
      - PROCESSOR_DEVICE=cuda
      - GLINER_MODEL=urchade/gliner_medium-v2.1
      - EMBED_MODEL=Alibaba-NLP/gte-Qwen3-Embedding-0.6B
      - SURYA_BATCH_SIZE=4
      - SURYA_LANGS=de,en
      - LANCEDB_PATH=/data/lancedb
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      start_period: 300s
      retries: 3
    profiles:
      - gpu

  # ===========================================================================
  # DOCUMENT PROCESSOR CPU (Alternative ohne GPU)
  # ===========================================================================
  # Für Systeme ohne NVIDIA GPU
  # Langsamer, aber funktional identisch
  # Aktivieren mit: docker compose --profile cpu up -d
  document-processor-cpu:
    build:
      context: ./infra/docker/document-processor
      dockerfile: Dockerfile.cpu
    image: conductor-document-processor:cpu
    container_name: conductor-document-processor
    restart: unless-stopped
    ports:
      - "8005:8000"
    volumes:
      - F:/:/mnt/data:ro
      - huggingface_cache:/root/.cache/huggingface
      - lancedb_data:/data/lancedb
    networks:
      - conductor-net
    environment:
      - PROCESSOR_DEVICE=cpu
      - GLINER_MODEL=urchade/gliner_small-v2.1
      - EMBED_MODEL=Alibaba-NLP/gte-Qwen3-Embedding-0.6B
      - SURYA_BATCH_SIZE=1
      - SURYA_LANGS=de,en
      - LANCEDB_PATH=/data/lancedb
      - OMP_NUM_THREADS=4
    deploy:
      resources:
        limits:
          memory: 6G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      start_period: 600s
      retries: 3
    profiles:
      - cpu

  # ===========================================================================
  # NEURAL WORKER (LEGACY - wird durch document-processor ersetzt)
  # ===========================================================================
  # DEPRECATED: Verwende document-processor stattdessen
  # Nur für Kompatibilität mit --profile legacy
  neural-worker:
    build:
      context: ./infra/docker/neural-worker
      dockerfile: Dockerfile
    image: conductor-neural-worker:latest
    container_name: conductor-neural-worker
    restart: unless-stopped
    ports:
      - "8006:8000"  # Anderer Port um Konflikt zu vermeiden
    volumes:
      - F:/:/mnt/data:ro
      - huggingface_cache:/root/.cache/huggingface
      - lancedb_data:/lancedb_storage
    networks:
      - conductor-net
    environment:
      - GLINER_MODEL=urchade/gliner_medium-v2.1
      - LANCEDB_PATH=/lancedb_storage
    profiles:
      - legacy  # Nur mit --profile legacy starten
    deploy:
      resources:
        limits:
          memory: 4G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      retries: 3

  # ===========================================================================
  # INTELLIGENCE PIPELINE
  # ===========================================================================

  # Universal Router - Magic Byte Detection → Queue Assignment
  # Port: 8030
  universal-router:
    build:
      context: ./infra/docker/universal-router
      dockerfile: Dockerfile
    image: conductor-universal-router:latest
    container_name: conductor-universal-router
    restart: unless-stopped
    ports:
      - "8030:8030"
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    depends_on:
      - redis
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8030/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Orchestrator - Priority Queues & Job Distribution
  # Port: 8020
  orchestrator:
    build:
      context: ./infra/docker/orchestrator
      dockerfile: Dockerfile
    image: conductor-orchestrator:latest
    container_name: conductor-orchestrator
    restart: unless-stopped
    ports:
      - "8020:8020"
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    depends_on:
      - redis
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8020/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # TIERED EXTRACTION WORKERS (Intelligence-Grade)
  # ===========================================================================
  # Horizontally scalable workers per file type
  # Scale with: docker compose up --scale worker-documents=8

  # Documents (PDF, DOCX, XLSX, TXT, HTML)
  worker-documents:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - DOCUMENT_PROCESSOR_URL=http://surya-ocr:8000
      - TIKA_URL=http://tika:9998
      - WORKER_TYPE=documents
      - INPUT_QUEUE=extract:documents
      - CONSUMER_GROUP=workers-documents
    depends_on:
      - redis
      - tika
    deploy:
      replicas: 4
      resources:
        limits:
          memory: 512M
    profiles:
      - intelligence

  # E-Books (EPUB, MOBI, AZW, AZW3, DjVu)
  worker-ebooks:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - EBOOK_PARSER_URL=http://ebook-parser:8000
      - WORKER_TYPE=ebooks
      - INPUT_QUEUE=extract:ebooks
      - CONSUMER_GROUP=workers-ebooks
    depends_on:
      - redis
      - ebook-parser
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 512M
    profiles:
      - intelligence

  # Audio (MP3, WAV, M4A, FLAC)
  worker-audio:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - WHISPERX_URL=http://whisperx:9000
      - WORKER_TYPE=audio
      - INPUT_QUEUE=extract:audio
      - CONSUMER_GROUP=workers-audio
    depends_on:
      - redis
      - whisperx
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 1G
    profiles:
      - intelligence

  # Images (JPG, PNG, TIFF) - OCR
  worker-images:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - DOCUMENT_PROCESSOR_URL=http://surya-ocr:8000
      - WORKER_TYPE=images
      - INPUT_QUEUE=extract:images
      - CONSUMER_GROUP=workers-images
    depends_on:
      - redis
      - surya-ocr
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1G
    profiles:
      - intelligence

  # Metadata (RAW, PSD, EXE, AI, XCF, ICO)
  worker-metadata:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - METADATA_EXTRACTOR_URL=http://metadata-extractor:8000
      - WORKER_TYPE=metadata
      - CONSUMER_GROUP=workers-metadata
    depends_on:
      - redis
      - metadata-extractor
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 512M
    profiles:
      - intelligence

  # Video (MP4, MKV, AVI)
  worker-video:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - WHISPERX_URL=http://whisperx:9000
      - WORKER_TYPE=video
      - INPUT_QUEUE=extract:video
      - CONSUMER_GROUP=workers-video
    depends_on:
      - redis
      - whisperx
    deploy:
      replicas: 1
      resources:
        limits:
          memory: 2G
    profiles:
      - intelligence

  # Email (EML, MSG)
  worker-email:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - WORKER_TYPE=email
      - INPUT_QUEUE=extract:email
      - CONSUMER_GROUP=workers-email
    depends_on:
      - redis
    deploy:
      replicas: 1
      resources:
        limits:
          memory: 512M
    profiles:
      - intelligence

  # Archives (ZIP, RAR, 7Z, TAR.GZ)
  worker-archive:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - WORKER_TYPE=archive
      - INPUT_QUEUE=extract:archive
      - CONSUMER_GROUP=workers-archive
    depends_on:
      - redis
    deploy:
      replicas: 1
      resources:
        limits:
          memory: 512M
    profiles:
      - intelligence

  # Legacy single worker (for backwards compatibility)
  extraction-worker:
    build:
      context: ./infra/docker/workers
      dockerfile: Dockerfile
    image: conductor-extraction-worker:latest
    restart: unless-stopped
    volumes:
      - F:/:/mnt/data:ro
    networks:
      - conductor-net
    environment:
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - DOCUMENT_PROCESSOR_URL=http://document-processor:8000
      - TIKA_URL=http://tika:9998
      - WHISPERX_URL=http://whisperx:9000
      - WORKER_TYPE=documents
      - CONSUMER_GROUP=extraction-workers
    depends_on:
      - redis
      - tika
    deploy:
      resources:
        limits:
          memory: 512M
    profiles:
      - gpu

  # ===========================================================================
  # CONDUCTOR API (Central Intelligence Service)
  # ===========================================================================
  # Zentraler API-Endpunkt mit:
  # - Qdrant/RAG Suche
  # - Tika HTML->Markdown Extraktion
  # - Context Headers für RAG
  # - Feedback Tracking für Klassifikations-Korrekturen
  # ===========================================================================
  conductor-api:
    build:
      context: ./infra/docker/conductor-api
      dockerfile: Dockerfile
    image: conductor-api:latest
    container_name: conductor-api
    restart: unless-stopped
    ports:
      - "8010:8000"
    volumes:
      # - F:/:/mnt/data:ro  # TEMP DISABLED: Docker Desktop mount bug
      - conductor_api_data:/data
    networks:
      - conductor-net
    environment:
      - TIKA_URL=http://tika:9998/tika
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_API_KEY=${QDRANT_API_KEY}
      - OLLAMA_URL=http://ollama:11434
      - DATA_DIR=/data
    depends_on:
      - tika
      - redis
      - qdrant
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.api.rule=Host(`api.local`)"
      - "traefik.http.services.api.loadbalancer.server.port=8000"

  # ===========================================================================
  # PERPLEXICA RAG UI
  # ===========================================================================

  perplexica:
    build:
      context: ./ui/perplexica
      dockerfile: Dockerfile
    image: perplexica-neural-vault:latest
    container_name: conductor-perplexica
    restart: unless-stopped
    ports:
      - "3100:${PERPLEXICA_PORT:-3000}"    # Main UI
      - "8180:8080"    # SearxNG (internal)
    volumes:
      - perplexica_data:/home/perplexica/data
    networks:
      - conductor-net
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434          # For Perplexica auto-config
      - NEURAL_VAULT_API_URL=http://conductor-api:8010
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_API_KEY=${QDRANT_API_KEY}
      - SEARXNG_API_URL=http://localhost:8080        # Internal SearxNG
      - DOCKER=true                                   # Tells Perplexica it's in Docker
    depends_on:
      - ollama
      - qdrant
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${PERPLEXICA_PORT:-3000}"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # NEURAL SEARCH API (RAG + LLM Synthesis)
  # ===========================================================================

  neural-search-api:
    build:
      context: ./infra/docker/neural-search-api
      dockerfile: Dockerfile
    image: neural-search-api:latest
    container_name: conductor-neural-search
    restart: unless-stopped
    ports:
      - "8040:8040"
    networks:
      - conductor-net
    environment:
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_API_KEY=${QDRANT_API_KEY}
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      - QDRANT_URL=http://qdrant:6333
    depends_on:
      - qdrant
      - redis
      - ollama
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8040/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.neural.rule=Host(`neural.local`)"
      - "traefik.http.services.neural.loadbalancer.server.port=8040"

  # ===========================================================================
  # NEXTCLOUD (File Sync)
  # ===========================================================================

  nextcloud:
    image: nextcloud:30-apache
    container_name: conductor-nextcloud
    restart: unless-stopped
    environment:
      - MYSQL_HOST=nextcloud-db
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=${NC_DB_USER:-nextcloud}
      - MYSQL_PASSWORD=${NC_DB_PASSWORD:?NC_DB_PASSWORD must be set}  # ✅ REQUIRED
      - NEXTCLOUD_ADMIN_USER=${NC_ADMIN_USER:-admin}
      - NEXTCLOUD_ADMIN_PASSWORD=${NC_ADMIN_PASSWORD:?NC_ADMIN_PASSWORD must be set}  # ✅ REQUIRED
      - NEXTCLOUD_TRUSTED_DOMAINS=localhost nextcloud.local 192.168.1.*
      - OVERWRITEPROTOCOL=http
    volumes:
      - nextcloud_data:/var/www/html
      - F:/:/mnt/datapool  # External Storage
    ports:
      - "8081:80"
    networks:
      - conductor-net
    depends_on:
      - nextcloud-db
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.nextcloud.rule=Host(`cloud.local`)"
      - "traefik.http.services.nextcloud.loadbalancer.server.port=80"
    deploy:
      resources:
        limits:
          memory: 512M

  nextcloud-db:
    image: mariadb:11
    container_name: conductor-nextcloud-db
    restart: unless-stopped
    environment:
      - MYSQL_ROOT_PASSWORD=${NC_DB_ROOT_PASSWORD:?NC_DB_ROOT_PASSWORD must be set}  # ✅ REQUIRED
      - MYSQL_DATABASE=nextcloud
      - MYSQL_USER=${NC_DB_USER:-nextcloud}
      - MYSQL_PASSWORD=${NC_DB_PASSWORD}
    volumes:
      - mariadb_data:/var/lib/mysql
    networks:
      - conductor-net
    deploy:
      resources:
        limits:
          memory: 256M

# =============================================================================
# NETWORKS & VOLUMES
# =============================================================================

networks:
  conductor-net:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  n8n_data:
  qdrant_data:
  perplexica_data:     # NEW: Perplexica RAG UI data
  ollama_data:
  nextcloud_data:
  mariadb_data:
  # Binary Processing
  whisper_models:
  ffmpeg_temp:
  paddleocr_models:
  huggingface_cache:
  lancedb_data:
  # Conductor API
  conductor_api_data:
